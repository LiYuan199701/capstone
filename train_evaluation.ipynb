{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_evaluation.ipynb","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyPdhQe9kezZZYuC5eVq2GN6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Define the PyTorch models"],"metadata":{"id":"xDo_L4vuYhl_"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"8kV4qSgUXMvA","executionInfo":{"status":"ok","timestamp":1648831487884,"user_tz":300,"elapsed":6123,"user":{"displayName":"Li Yuan","userId":"05474779963952877698"}}},"outputs":[],"source":["'''\n","Adapted from kuangliu/pytorch-cifar .\n","'''\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(\n","            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        \n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n","                               stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion *\n","                               planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, in_channels=1, num_classes=2):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.linear = nn.Linear(512 * block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = self.avgpool(out)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18(in_channels, num_classes):\n","    return ResNet(BasicBlock, [2, 2, 2, 2], in_channels=in_channels, num_classes=num_classes)\n","\n","\n","def ResNet50(in_channels, num_classes):\n","    return ResNet(Bottleneck, [3, 4, 6, 3], in_channels=in_channels, num_classes=num_classes)"]},{"cell_type":"markdown","source":["# Define training and evalution functions"],"metadata":{"id":"HqrnuLgTYoHF"}},{"cell_type":"code","source":["!pip install torch torchvision\n","!pip install tensorboard\n","!pip install medmnist\n","import medmnist\n","print(medmnist.__version__)"],"metadata":{"id":"xaI6oZbtZfXR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import argparse\n","import time\n","from tqdm import trange\n","import numpy as np\n","import PIL\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as data\n","import torchvision.transforms as transforms\n","from torchvision.models import resnet18, resnet50\n","#from tensorboardX import SummaryWriter\n","from collections import OrderedDict\n","#from models import ResNet18, ResNet50\n","\n","import medmnist\n","from medmnist import INFO, Evaluator\n","\n","\n","def main(data_flag, output_root, num_epochs, gpu_ids, batch_size, download, model_flag, resize, as_rgb, model_path, run):\n","\n","    lr = 0.001\n","    gamma=0.1\n","    milestones = [0.5 * num_epochs, 0.75 * num_epochs]\n","\n","    info = INFO[data_flag]\n","    task = info['task']\n","    n_channels = 3 if as_rgb else info['n_channels']\n","    n_classes = len(info['label'])\n","\n","    DataClass = getattr(medmnist, info['python_class'])\n","\n","    str_ids = gpu_ids.split(',')\n","    gpu_ids = []\n","    for str_id in str_ids:\n","        id = int(str_id)\n","        if id >= 0:\n","            gpu_ids.append(id)\n","    if len(gpu_ids) > 0:\n","        os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(gpu_ids[0])\n","\n","    device = torch.device('cuda:{}'.format(gpu_ids[0])) if gpu_ids else torch.device('cpu') \n","    \n","    output_root = os.path.join(output_root, data_flag, time.strftime(\"%y%m%d_%H%M%S\"))\n","    if not os.path.exists(output_root):\n","        os.makedirs(output_root)\n","\n","    print('==> Preparing data...')\n","\n","    if resize:\n","        data_transform = transforms.Compose(\n","            [transforms.Resize((224, 224), interpolation=PIL.Image.NEAREST), \n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[.5], std=[.5])])\n","    else:\n","        data_transform = transforms.Compose(\n","            [transforms.ToTensor(),\n","            transforms.Normalize(mean=[.5], std=[.5])])\n","     \n","    train_dataset = DataClass(split='train', transform=data_transform, download=download, as_rgb=as_rgb)\n","    val_dataset = DataClass(split='val', transform=data_transform, download=download, as_rgb=as_rgb)\n","    test_dataset = DataClass(split='test', transform=data_transform, download=download, as_rgb=as_rgb)\n","\n","    \n","    train_loader = data.DataLoader(dataset=train_dataset,\n","                                batch_size=batch_size,\n","                                shuffle=True)\n","    train_loader_at_eval = data.DataLoader(dataset=train_dataset,\n","                                batch_size=batch_size,\n","                                shuffle=False)\n","    val_loader = data.DataLoader(dataset=val_dataset,\n","                                batch_size=batch_size,\n","                                shuffle=False)\n","    test_loader = data.DataLoader(dataset=test_dataset,\n","                                batch_size=batch_size,\n","                                shuffle=False)\n","\n","    print('==> Building and training model...')\n","    \n","    \n","    if model_flag == 'resnet18':\n","        model =  resnet18(pretrained=False, num_classes=n_classes) if resize else ResNet18(in_channels=n_channels, num_classes=n_classes)\n","    elif model_flag == 'resnet50':\n","        model =  resnet50(pretrained=False, num_classes=n_classes) if resize else ResNet50(in_channels=n_channels, num_classes=n_classes)\n","    else:\n","        raise NotImplementedError\n","\n","    model = model.to(device)\n","\n","    train_evaluator = medmnist.Evaluator(data_flag, 'train')\n","    val_evaluator = medmnist.Evaluator(data_flag, 'val')\n","    test_evaluator = medmnist.Evaluator(data_flag, 'test')\n","\n","    if task == \"multi-label, binary-class\":\n","        criterion = nn.BCEWithLogitsLoss()\n","    else:\n","        criterion = nn.CrossEntropyLoss()\n","\n","    if model_path is not None:\n","        model.load_state_dict(torch.load(model_path, map_location=device)['net'], strict=True)\n","        train_metrics = test(model, train_evaluator, train_loader_at_eval, task, criterion, device, run, output_root)\n","        val_metrics = test(model, val_evaluator, val_loader, task, criterion, device, run, output_root)\n","        test_metrics = test(model, test_evaluator, test_loader, task, criterion, device, run, output_root)\n","\n","        print('train  auc: %.5f  acc: %.5f\\n' % (train_metrics[1], train_metrics[2]) + \\\n","              'val  auc: %.5f  acc: %.5f\\n' % (val_metrics[1], val_metrics[2]) + \\\n","              'test  auc: %.5f  acc: %.5f\\n' % (test_metrics[1], test_metrics[2]))\n","\n","    if num_epochs == 0:\n","        return\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n","\n","    logs = ['loss', 'auc', 'acc']\n","    train_logs = ['train_'+log for log in logs]\n","    val_logs = ['val_'+log for log in logs]\n","    test_logs = ['test_'+log for log in logs]\n","    log_dict = OrderedDict.fromkeys(train_logs+val_logs+test_logs, 0)\n","    \n","    writer = SummaryWriter(log_dir=os.path.join(output_root, 'Tensorboard_Results'))\n","\n","    best_auc = 0\n","    best_epoch = 0\n","    best_model = model\n","\n","    global iteration\n","    iteration = 0\n","    \n","    for epoch in trange(num_epochs):        \n","        train_loss = train(model, train_loader, task, criterion, optimizer, device, writer)\n","        \n","        train_metrics = test(model, train_evaluator, train_loader_at_eval, task, criterion, device, run)\n","        val_metrics = test(model, val_evaluator, val_loader, task, criterion, device, run)\n","        test_metrics = test(model, test_evaluator, test_loader, task, criterion, device, run)\n","        \n","        scheduler.step()\n","        \n","        for i, key in enumerate(train_logs):\n","            log_dict[key] = train_metrics[i]\n","        for i, key in enumerate(val_logs):\n","            log_dict[key] = val_metrics[i]\n","        for i, key in enumerate(test_logs):\n","            log_dict[key] = test_metrics[i]\n","\n","        for key, value in log_dict.items():\n","            writer.add_scalar(key, value, epoch)\n","            \n","        cur_auc = val_metrics[1]\n","        if cur_auc > best_auc:\n","            best_epoch = epoch\n","            best_auc = cur_auc\n","            best_model = model\n","            print('cur_best_auc:', best_auc)\n","            print('cur_best_epoch', best_epoch)\n","\n","    state = {\n","        'net': best_model.state_dict(),\n","    }\n","\n","    path = os.path.join(output_root, 'best_model.pth')\n","    torch.save(state, path)\n","\n","    train_metrics = test(best_model, train_evaluator, train_loader_at_eval, task, criterion, device, run, output_root)\n","    val_metrics = test(best_model, val_evaluator, val_loader, task, criterion, device, run, output_root)\n","    test_metrics = test(best_model, test_evaluator, test_loader, task, criterion, device, run, output_root)\n","\n","    train_log = 'train  auc: %.5f  acc: %.5f\\n' % (train_metrics[1], train_metrics[2])\n","    val_log = 'val  auc: %.5f  acc: %.5f\\n' % (val_metrics[1], val_metrics[2])\n","    test_log = 'test  auc: %.5f  acc: %.5f\\n' % (test_metrics[1], test_metrics[2])\n","\n","    log = '%s\\n' % (data_flag) + train_log + val_log + test_log\n","    print(log)\n","            \n","    with open(os.path.join(output_root, '%s_log.txt' % (data_flag)), 'a') as f:\n","        f.write(log)  \n","            \n","    writer.close()\n","\n","\n","def train(model, train_loader, task, criterion, optimizer, device, writer):\n","    total_loss = []\n","    global iteration\n","\n","    model.train()\n","    for batch_idx, (inputs, targets) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","        outputs = model(inputs.to(device))\n","\n","        if task == 'multi-label, binary-class':\n","            targets = targets.to(torch.float32).to(device)\n","            loss = criterion(outputs, targets)\n","        else:\n","            targets = torch.squeeze(targets, 1).long().to(device)\n","            loss = criterion(outputs, targets)\n","\n","        total_loss.append(loss.item())\n","        writer.add_scalar('train_loss_logs', loss.item(), iteration)\n","        iteration += 1\n","\n","        loss.backward()\n","        optimizer.step()\n","    \n","    epoch_loss = sum(total_loss)/len(total_loss)\n","    return epoch_loss\n","\n","\n","def test(model, evaluator, data_loader, task, criterion, device, run, save_folder=None):\n","\n","    model.eval()\n","    \n","    total_loss = []\n","    y_score = torch.tensor([]).to(device)\n","\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(data_loader):\n","            outputs = model(inputs.to(device))\n","            \n","            if task == 'multi-label, binary-class':\n","                targets = targets.to(torch.float32).to(device)\n","                loss = criterion(outputs, targets)\n","                m = nn.Sigmoid()\n","                outputs = m(outputs).to(device)\n","            else:\n","                targets = torch.squeeze(targets, 1).long().to(device)\n","                loss = criterion(outputs, targets)\n","                m = nn.Softmax(dim=1)\n","                outputs = m(outputs).to(device)\n","                targets = targets.float().resize_(len(targets), 1)\n","\n","            total_loss.append(loss.item())\n","            y_score = torch.cat((y_score, outputs), 0)\n","\n","        y_score = y_score.detach().cpu().numpy()\n","        auc, acc = evaluator.evaluate(y_score, save_folder, run)\n","        \n","        test_loss = sum(total_loss) / len(total_loss)\n","\n","        return [test_loss, auc, acc]"],"metadata":{"id":"-qFl3NOhYIik","executionInfo":{"status":"ok","timestamp":1648831882172,"user_tz":300,"elapsed":789,"user":{"displayName":"Li Yuan","userId":"05474779963952877698"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    parser = argparse.ArgumentParser(\n","        description='RUN Baseline model of MedMNIST2D')\n","\n","    parser.add_argument('--data_flag',\n","                        default='pathmnist',\n","                        type=str)\n","    parser.add_argument('--output_root',\n","                        default='./output',\n","                        help='output root, where to save models and results',\n","                        type=str)\n","    parser.add_argument('--num_epochs',\n","                        default=100,\n","                        help='num of epochs of training, the script would only test model if set num_epochs to 0',\n","                        type=int)\n","    parser.add_argument('--gpu_ids',\n","                        default='0',\n","                        type=str)\n","    parser.add_argument('--batch_size',\n","                        default=128,\n","                        type=int)\n","    parser.add_argument('--download',\n","                        action=\"store_true\")\n","    parser.add_argument('--resize',\n","                        help='resize images of size 28x28 to 224x224',\n","                        action=\"store_true\")\n","    parser.add_argument('--as_rgb',\n","                        help='convert the grayscale image to RGB',\n","                        action=\"store_true\")\n","    parser.add_argument('--model_path',\n","                        default=None,\n","                        help='root of the pretrained model to test',\n","                        type=str)\n","    parser.add_argument('--model_flag',\n","                        default='resnet18',\n","                        help='choose backbone from resnet18, resnet50',\n","                        type=str)\n","    parser.add_argument('--run',\n","                        default='model1',\n","                        help='to name a standard evaluation csv file, named as {flag}_{split}_[AUC]{auc:.3f}_[ACC]{acc:.3f}@{run}.csv',\n","                        type=str)\n","\n","\n","    args = parser.parse_args()\n","    data_flag = args.data_flag\n","    output_root = args.output_root\n","    num_epochs = args.num_epochs\n","    gpu_ids = args.gpu_ids\n","    batch_size = args.batch_size\n","    download = args.download\n","    model_flag = args.model_flag\n","    resize = args.resize\n","    as_rgb = args.as_rgb\n","    model_path = args.model_path\n","    run = args.run\n","    \n","    main(data_flag, output_root, num_epochs, gpu_ids, batch_size, download, model_flag, resize, as_rgb, model_path, run)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":276},"id":"Zhdc6HdSYes0","executionInfo":{"status":"error","timestamp":1648832130024,"user_tz":300,"elapsed":234,"user":{"displayName":"Li Yuan","userId":"05474779963952877698"}},"outputId":"a8074a9d-18c9-4f74-f7b7-7822821f5ff6"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["usage: ipykernel_launcher.py [-h] [--data_flag DATA_FLAG]\n","                             [--output_root OUTPUT_ROOT]\n","                             [--num_epochs NUM_EPOCHS] [--gpu_ids GPU_IDS]\n","                             [--batch_size BATCH_SIZE] [--download] [--resize]\n","                             [--as_rgb] [--model_path MODEL_PATH]\n","                             [--model_flag MODEL_FLAG] [--run RUN]\n","ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-13c58776-08dc-4c42-9072-9fd95c509c38.json\n"]},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"]}]}]}